# ğŸ® Projet Reinforcement Learning - GridWorld

## ğŸ“‹ Table des matiÃ¨res
1. [Introduction](#introduction)
2. [Architecture du projet](#architecture-du-projet)
3. [Concepts thÃ©oriques](#concepts-thÃ©oriques)
4. [Environnement GridWorld](#environnement-gridworld)
5. [Agents implÃ©mentÃ©s](#agents-implÃ©mentÃ©s)
6. [Installation et utilisation](#installation-et-utilisation)
7. [RÃ©sultats et comparaisons](#rÃ©sultats-et-comparaisons)
8. [ExpÃ©rimentations avancÃ©es](#expÃ©rimentations-avancÃ©es)

---

## ğŸ¯ Introduction

Ce projet est une implÃ©mentation complÃ¨te de plusieurs algorithmes d'apprentissage par renforcement (Reinforcement Learning) appliquÃ©s Ã  un environnement GridWorld. Il compare trois approches fondamentales :

- **Value Iteration** (Programmation Dynamique)
- **V-Learning** (Temporal Difference avec fonction V)
- **Q-Learning** (Temporal Difference avec fonction Q)

### Objectif pÃ©dagogique
Comprendre les diffÃ©rences entre les algorithmes model-based et model-free, et observer comment les agents apprennent Ã  naviguer dans un environnement avec obstacles pour atteindre un objectif.

---

## ğŸ“ Architecture du projet

```
RL_Gridworld/
â”‚
â”œâ”€â”€ grid_env.py              # Environnement GridWorld (hÃ©rite de gym.Env)
â”œâ”€â”€ random_agent.py          # Agent alÃ©atoire (baseline)
â”œâ”€â”€ optimal_agent.py         # Value Iteration (DP)
â”œâ”€â”€ v_learning_agent.py      # V-Learning (TD)
â”œâ”€â”€ q_learning_agent.py      # Q-Learning (TD)
â”‚
â”œâ”€â”€ main.py                  # Script principal (Value Iteration + V-Learning)
â”œâ”€â”€ main_qlearning.py        # Comparaison des 3 algorithmes
â”œâ”€â”€ dynamic_goal_experiment.py  # ExpÃ©rience avec goal dynamique
â”‚
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â””â”€â”€ README.md               # Documentation (ce fichier)
```

---

## ğŸ“š Concepts thÃ©oriques

### 1. Apprentissage par Renforcement (RL)

L'apprentissage par renforcement est un paradigme d'apprentissage automatique oÃ¹ un **agent** apprend Ã  prendre des **actions** dans un **environnement** pour maximiser une **rÃ©compense cumulative**.

**Composants clÃ©s :**
- **Ã‰tat (State)** : Situation actuelle de l'agent
- **Action (Action)** : Choix que l'agent peut faire
- **RÃ©compense (Reward)** : Signal de feedback de l'environnement
- **Politique (Policy)** : StratÃ©gie de l'agent (quelle action choisir dans chaque Ã©tat)
- **Fonction de valeur** : Estimation de la rÃ©compense future attendue

### 2. Processus de DÃ©cision Markovien (MDP)

Un MDP est dÃ©fini par :
- **S** : Ensemble d'Ã©tats
- **A** : Ensemble d'actions
- **P(s'|s,a)** : ProbabilitÃ© de transition de l'Ã©tat s vers s' en effectuant l'action a
- **R(s,a,s')** : RÃ©compense reÃ§ue lors de la transition
- **Î³ (gamma)** : Facteur de discount (0 â‰¤ Î³ â‰¤ 1)

### 3. Ã‰quation de Bellman

L'Ã©quation fondamentale du RL qui exprime la valeur d'un Ã©tat :

```
V(s) = max[R(s,a) + Î³ Î£ P(s'|s,a) V(s')]
        a            s'
```

Pour Q-Learning :
```
Q(s,a) = R(s,a) + Î³ Î£ P(s'|s,a) max Q(s',a')
                    s'           a'
```

---

## ğŸŒ Environnement GridWorld

### Description

`grid_env.py` - Classe **GridWorldEnv** hÃ©ritant de `gym.Env` (Gymnasium)

**CaractÃ©ristiques :**
- Grille 2D de taille configurable (par dÃ©faut 5Ã—5)
- Position de dÃ©part : (0,0)
- Obstacles : Positions infranchissables
- Goal : Objectif Ã  atteindre
- Actions : 4 directions (Haut, Bas, Gauche, Droite)

### Espace d'observation
```python
observation_space = spaces.Discrete(size * size)
```
Chaque case de la grille correspond Ã  un Ã©tat unique (0 Ã  24 pour une grille 5Ã—5).

### Espace d'action
```python
action_space = spaces.Discrete(4)
```
- 0 : Haut (â†‘)
- 1 : Bas (â†“)
- 2 : Gauche (â†)
- 3 : Droite (â†’)

### Fonction de rÃ©compense
```python
reward = -1  # PÃ©nalitÃ© pour chaque pas (encourage les chemins courts)
reward = 10  # RÃ©compense pour atteindre le goal
```

### Dynamique de transition
- Si l'agent tape un **mur** ou un **obstacle**, il reste sur place
- Sinon, il se dÃ©place dans la direction choisie
- L'Ã©pisode se termine quand le goal est atteint (`terminated = True`)

### MÃ©thodes principales

```python
reset() â†’ observation, info
step(action) â†’ observation, reward, terminated, truncated, info
set_goal(new_goal) â†’ None  # Change la position du goal dynamiquement
```

---

## ğŸ¤– Agents implÃ©mentÃ©s

### 1. Random Agent (Agent AlÃ©atoire)

**Fichier :** `random_agent.py`

**Principe :**
- SÃ©lectionne une action alÃ©atoire Ã  chaque pas
- Sert de **baseline** pour comparer les performances

**Code :**
```python
def act(self, observation, env):
    return env.action_space.sample()
```

**Performance :** TrÃ¨s faible, met beaucoup de temps (voire ne termine jamais)

---

### 2. Value Iteration Agent

**Fichier :** `optimal_agent.py`

#### Principe thÃ©orique

**Type :** Programmation Dynamique (Model-Based)

**Ce qu'il apprend :** Fonction de valeur **V(s)** pour chaque Ã©tat

**Formule de mise Ã  jour :**
```
V(s) â† max[R(s,a) + Î³ V(s')]
        a
```

**Algorithme :**
1. Initialiser V(s) = 0 pour tous les Ã©tats
2. **ItÃ©rer jusqu'Ã  convergence :**
   - Pour chaque Ã©tat s :
     - Pour chaque action a :
       - Calculer la valeur : R + Î³ V(s')
     - V(s) = max de ces valeurs
   - Calculer delta = max|V_ancien - V_nouveau|
   - Si delta < Î¸ (seuil), arrÃªter
3. Extraire la politique : Ï€(s) = argmax[R + Î³ V(s')]

#### CaractÃ©ristiques

âœ… **Avantages :**
- Trouve la solution **optimale**
- Converge rapidement (9 itÃ©rations dans notre cas)
- Pas besoin d'exploration

âŒ **InconvÃ©nients :**
- NÃ©cessite un **modÃ¨le complet** de l'environnement (transitions, rÃ©compenses)
- Doit simuler toutes les actions pour choisir
- Non applicable aux environnements inconnus

#### Features apprises
**V(Ã©tat)** â†’ "Quelle est la valeur d'Ãªtre dans cet Ã©tat ?"

Exemple : V[(2,3)] = 5.8 signifie "Ãªtre en position (2,3) vaut 5.8"

---

### 3. V-Learning Agent (TD Learning)

**Fichier :** `v_learning_agent.py`

#### Principe thÃ©orique

**Type :** Temporal Difference Learning (Model-Free)

**Ce qu'il apprend :** Fonction de valeur **V(s)** par expÃ©rience

**Formule de mise Ã  jour :**
```
V(s) â† V(s) + Î±[R + Î³ V(s') - V(s)]
                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                   TD target
```

OÃ¹ :
- **Î± (alpha)** : Taux d'apprentissage (0.1)
- **Î³ (gamma)** : Facteur de discount (0.99)
- **TD target** : Estimation basÃ©e sur l'expÃ©rience rÃ©elle

**Algorithme :**
1. Initialiser V(s) = 0 pour tous les Ã©tats
2. **Pour chaque Ã©pisode :**
   - Partir de l'Ã©tat initial
   - **Pour chaque pas :**
     - Choisir action avec Îµ-greedy (exploration/exploitation)
     - ExÃ©cuter l'action, observer R et s'
     - Mettre Ã  jour : V(s) â† V(s) + Î±[R + Î³V(s') - V(s)]
     - s â† s'
   - Terminer si goal atteint

#### Politique Îµ-greedy

```python
if random() < Îµ:
    action = random()  # Exploration (10%)
else:
    action = argmax Q_estimÃ©  # Exploitation (90%)
```

#### CaractÃ©ristiques

âœ… **Avantages :**
- **Model-free** : Apprend par interaction
- S'adapte aux environnements inconnus
- Apprentissage en ligne

âŒ **InconvÃ©nients :**
- Converge plus lentement que Value Iteration
- NÃ©cessite beaucoup d'Ã©pisodes (1000+)
- Doit **simuler** toutes les actions pour choisir (comme Value Iteration)

#### Features apprises
**V(Ã©tat)** â†’ MÃªme que Value Iteration, mais appris par expÃ©rience

---

### 4. Q-Learning Agent

**Fichier :** `q_learning_agent.py`

#### Principe thÃ©orique

**Type :** Temporal Difference Learning (Model-Free)

**Ce qu'il apprend :** Fonction Q-valeur **Q(s, a)** pour chaque paire Ã©tat-action

**Formule de mise Ã  jour :**
```
Q(s,a) â† Q(s,a) + Î±[R + Î³ max Q(s',a') - Q(s,a)]
                           a'
```

**DiffÃ©rence clÃ© avec V-Learning :**
- V-Learning : V(Ã©tat) â†’ 25 valeurs (grille 5Ã—5)
- Q-Learning : Q(Ã©tat, action) â†’ 25 Ã— 4 = **100 valeurs**

**Algorithme :**
1. Initialiser Q(s,a) = 0 pour tous les Ã©tats et actions
2. **Pour chaque Ã©pisode :**
   - Partir de l'Ã©tat initial
   - **Pour chaque pas :**
     - Choisir action avec Îµ-greedy
     - ExÃ©cuter l'action, observer R et s'
     - Mettre Ã  jour : Q(s,a) â† Q(s,a) + Î±[R + Î³ max Q(s',a') - Q(s,a)]
     - s â† s'
   - Terminer si goal atteint

#### CaractÃ©ristiques

âœ… **Avantages :**
- **Model-free** : Apprend par interaction
- **Action directe** : argmax Q(s,a) sans simulation !
- Off-policy : Peut apprendre d'expÃ©riences explorÃ©es diffÃ©remment
- Base de Deep Q-Learning (DQN)

âŒ **InconvÃ©nients :**
- Table Q plus grande (mÃ©moire)
- Converge plus lentement au dÃ©but
- NÃ©cessite beaucoup d'Ã©pisodes

#### Features apprises
**Q(Ã©tat, action)** â†’ "Quelle est la valeur de faire l'action a dans l'Ã©tat s ?"

Exemples :
- Q[(2,3), Haut] = 6.5
- Q[(2,3), Bas] = 2.1
- Q[(2,3), Gauche] = 4.0
- Q[(2,3), Droite] = 7.2 â† **Meilleure action !**

#### Extraction de V depuis Q
```python
V(s) = max Q(s,a)
       a
```

---

## ğŸš€ Installation et utilisation

### PrÃ©requis

- Python 3.8+
- pip

### Installation

1. **Cloner ou tÃ©lÃ©charger le projet**

2. **CrÃ©er un environnement virtuel (recommandÃ©)**
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

3. **Installer les dÃ©pendances**
```powershell
pip install -r requirements.txt
```

**DÃ©pendances :**
- gymnasium==1.2.2
- numpy==2.3.5
- matplotlib==3.10.7

### ExÃ©cution

#### Script 1 : Value Iteration + V-Learning
```powershell
python main.py
```

**Ce script exÃ©cute :**
1. Agent Random (baseline)
2. Value Iteration (convergence en 9 itÃ©rations)
3. V-Learning (1000 Ã©pisodes avec visualisations aux Ã©pisodes 1, 100, 500, 1000)

**Visualisations gÃ©nÃ©rÃ©es :**
- Convergence de Value Iteration
- Courbes d'apprentissage V-Learning
- Comparaison des tables V
- Heatmaps des valeurs
- Animation de l'agent se dÃ©plaÃ§ant

#### Script 2 : Comparaison des 3 algorithmes
```powershell
python main_qlearning.py
```

**Ce script compare :**
1. Value Iteration
2. V-Learning (1000 Ã©pisodes)
3. Q-Learning (1000 Ã©pisodes)

**Visualisations gÃ©nÃ©rÃ©es :**
- Courbes d'apprentissage comparÃ©es (V-Learning vs Q-Learning)
- Comparaison des 3 tables de valeurs
- Q-Table avec flÃ¨ches de politique optimale
- Statistiques de performance

#### Script 3 : ExpÃ©rience Goal Dynamique
```powershell
python dynamic_goal_experiment.py
```

**Ce script teste :**
- V-Learning avec goal qui change de position
- 2000 Ã©pisodes avec 4 changements de goal
- Visualisation de l'adaptation de l'agent

**Planning des changements :**
- Ã‰pisodes 0-499 : Goal Ã  (4,4)
- Ã‰pisodes 500-999 : Goal Ã  (0,4)
- Ã‰pisodes 1000-1499 : Goal Ã  (4,0)
- Ã‰pisodes 1500-1999 : Goal Ã  (2,3)

---

## ğŸ“Š RÃ©sultats et comparaisons

### Performance finale (Goal fixe Ã  (4,4))

| Algorithme | Convergence | Pas optimal | RÃ©compense | ModÃ¨le requis |
|------------|-------------|-------------|------------|---------------|
| **Value Iteration** | 9 itÃ©rations | 8 | +3.0 | âœ… Oui |
| **V-Learning** | 1000 Ã©pisodes | 8 | +3.0 | âŒ Non |
| **Q-Learning** | 1000 Ã©pisodes | 8 | +3.0 | âŒ Non |

### Vitesse de convergence

**Value Iteration :**
- âš¡ TrÃ¨s rapide (9 itÃ©rations)
- Calcul direct de la solution optimale

**V-Learning :**
- ğŸ¢ Apprentissage progressif
- RÃ©compense moyenne ~2.0 dÃ¨s le dÃ©but
- Stable tout au long de l'entraÃ®nement

**Q-Learning :**
- ğŸŒ DÃ©marrage lent (rÃ©compense -5.42 aux 100 premiers Ã©pisodes)
- Rattrape et stabilise autour de ~2.2
- Converge vers la politique optimale

### Taille des tables

**Grille 5Ã—5 :**
- Value Iteration : 25 valeurs (V-table)
- V-Learning : 25 valeurs (V-table)
- Q-Learning : **100 valeurs** (Q-table = 25 Ã©tats Ã— 4 actions)

### Avantages comparÃ©s

| CritÃ¨re | Value Iteration | V-Learning | Q-Learning |
|---------|----------------|------------|------------|
| OptimalitÃ© | â­â­â­ | â­â­ | â­â­ |
| Vitesse | â­â­â­ | â­ | â­ |
| Sans modÃ¨le | âŒ | âœ… | âœ… |
| Action directe | âŒ | âŒ | âœ… |
| MÃ©moire | â­â­â­ | â­â­â­ | â­â­ |
| ScalabilitÃ© | â­ | â­â­ | â­â­â­ |

---

## ğŸ”¬ ExpÃ©rimentations avancÃ©es

### 1. Goal Dynamique

**Observation :** Quand le goal change de position, l'agent doit rÃ©apprendre.

**Pourquoi ?** L'agent apprend seulement **V(position)** ou **Q(position, action)**, mais ne sait pas **oÃ¹ est le goal**.

**RÃ©sultats :**
- Ã€ chaque changement de goal : **chute temporaire** de performance
- Puis **rÃ©apprentissage rapide** grÃ¢ce Ã  la V-table existante
- La connaissance des obstacles est conservÃ©e

**Graphique :** Les lignes vertes montrent les changements de goal et les perturbations associÃ©es.

### 2. Feature Engineering

**ProblÃ¨me actuel :**
- Feature = Position de l'agent uniquement
- L'agent ne "voit" pas le goal

**AmÃ©lioration possible :**
- Feature = (Position agent, Position goal)
- L'agent apprendrait : "Comment aller de A vers B" (gÃ©nÃ©ralisation)
- S'adapterait **instantanÃ©ment** aux changements de goal

**Non implÃ©mentÃ© dans ce projet** (reste simple pour la pÃ©dagogie).

### 3. HyperparamÃ¨tres

**ParamÃ¨tres configurables :**

```python
epsilon = 0.1   # Taux d'exploration (10% actions alÃ©atoires)
alpha = 0.1     # Taux d'apprentissage
gamma = 0.99    # Facteur de discount (importance du futur)
```

**Effets :**
- â†‘ epsilon : Plus d'exploration, apprentissage plus lent mais robuste
- â†‘ alpha : Apprentissage plus rapide mais moins stable
- â†‘ gamma : Favorise les rÃ©compenses futures (chemins longs acceptables)

---

## ğŸ¨ Visualisations

### 1. Heatmaps des valeurs

**InterprÃ©tation :**
- Couleurs chaudes (jaune) : Ã‰tats de haute valeur (proches du goal)
- Couleurs froides (violet) : Ã‰tats de faible valeur (loin du goal)
- Les valeurs diminuent en s'Ã©loignant du goal

### 2. Courbes d'apprentissage

**RÃ©compenses vs Ã‰pisodes :**
- Montre la progression de l'agent
- Moyenne mobile pour lisser le bruit

**Pas vs Ã‰pisodes :**
- Nombre de pas pour atteindre le goal
- Diminue au fur et Ã  mesure de l'apprentissage

### 3. Q-Table avec flÃ¨ches

**Visualisation unique Ã  Q-Learning :**
- FlÃ¨ches colorÃ©es indiquent la meilleure action dans chaque case
- Rouge (â†‘), Bleu (â†“), Vert (â†), Violet (â†’)
- Montre visuellement la **politique optimale**

### 4. Animations

**Agents se dÃ©plaÃ§ant :**
- Cercle bleu : Value Iteration
- Cercle violet : V-Learning
- Cercle orange : Q-Learning
- Affichage en temps rÃ©el du dÃ©placement pas Ã  pas

---

## ğŸ§ª Notions clÃ©s apprises

### 1. Model-Based vs Model-Free

**Model-Based (Value Iteration) :**
- ConnaÃ®t les transitions P(s'|s,a)
- ConnaÃ®t les rÃ©compenses R(s,a)
- Calcule directement la solution optimale

**Model-Free (V-Learning, Q-Learning) :**
- Ne connaÃ®t PAS les transitions
- Apprend par **essai-erreur**
- S'adapte aux environnements inconnus

### 2. Exploration vs Exploitation

**Dilemme fondamental du RL :**
- **Exploration** : Essayer de nouvelles actions (dÃ©couvrir)
- **Exploitation** : Utiliser les meilleures actions connues (optimiser)

**Solution Îµ-greedy :**
```python
if random() < Îµ:
    explore()  # 10%
else:
    exploit()  # 90%
```

### 3. On-Policy vs Off-Policy

**On-Policy (V-Learning dans notre implÃ©mentation) :**
- Apprend de la politique qu'il suit

**Off-Policy (Q-Learning) :**
- Peut apprendre d'expÃ©riences diffÃ©rentes
- Met Ã  jour Q avec **max** mÃªme si l'action choisie Ã©tait exploratoire

### 4. Temporal Difference (TD)

**IdÃ©e clÃ© :** Mise Ã  jour basÃ©e sur la diffÃ©rence entre :
- **PrÃ©diction** : V(s) ou Q(s,a)
- **Cible** : R + Î³V(s') ou R + Î³ max Q(s',a')

**TD Error :**
```
Î´ = [R + Î³V(s')] - V(s)
```

C'est le "signal d'apprentissage".

---

## ğŸ“ˆ Extensions possibles

### 1. Deep Q-Learning (DQN)
Remplacer la Q-table par un **rÃ©seau de neurones** pour traiter des espaces d'Ã©tats continus ou trÃ¨s larges.

### 2. Policy Gradient
Apprendre directement la politique Ï€(a|s) au lieu de passer par Q ou V.

### 3. Actor-Critic
Combiner V-Learning (Critic) et Policy Gradient (Actor).

### 4. Multi-Agent RL
Plusieurs agents qui apprennent simultanÃ©ment dans le mÃªme environnement.

### 5. Environnements plus complexes
- StochasticitÃ© (transitions probabilistes)
- RÃ©compenses partielles
- Observations partielles (POMDP)

---

## ğŸ“ Conclusion

Ce projet illustre les fondamentaux de l'apprentissage par renforcement :

### Ce qu'on a appris :

1. **Environnement GridWorld** : CrÃ©ation d'un MDP avec Gymnasium
2. **Value Iteration** : Programmation dynamique pour trouver la solution optimale
3. **V-Learning** : Apprentissage de V(s) par Temporal Difference
4. **Q-Learning** : Apprentissage de Q(s,a) pour action directe
5. **Visualisations** : Comprendre visuellement comment l'agent apprend
6. **Comparaisons** : Model-based vs model-free, vitesse vs flexibilitÃ©

### Points clÃ©s :

- **Value Iteration** : Optimal mais nÃ©cessite un modÃ¨le
- **Q-Learning** : Plus lent mais gÃ©nÃ©ralise mieux (base de DQN)
- **Features** : L'agent apprend seulement ce qu'il observe
- **Exploration** : Essentielle pour dÃ©couvrir de nouvelles stratÃ©gies

### Applications rÃ©elles :

- Jeux (Atari, Go, Chess)
- Robotique (navigation, manipulation)
- Finance (trading algorithmique)
- Ressources (optimisation Ã©nergÃ©tique)
- PublicitÃ© (recommandation personnalisÃ©e)

---

## ğŸ“š RÃ©fÃ©rences

- **Sutton & Barto** - Reinforcement Learning: An Introduction (2018)
- **Gymnasium Documentation** - https://gymnasium.farama.org/
- **OpenAI Spinning Up** - https://spinningup.openai.com/

---

## ğŸ‘¨â€ğŸ’» Auteur

Projet rÃ©alisÃ© dans le cadre d'un cours d'apprentissage par renforcement.

**Date :** DÃ©cembre 2025

---

## ğŸ“ Licence

Ce projet est Ã  usage Ã©ducatif.

---

**Bon apprentissage ! ğŸš€**
